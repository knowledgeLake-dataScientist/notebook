{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import subprocess\n",
    "from tensorflowonspark import TFCluster\n",
    "import mnist_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.addPyFile(\"mnist_dist.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epochs\", help=\"number of epochs\", type=int, default=1)\n",
    "parser.add_argument(\"--images\", help=\"HDFS path to MNIST images in parallelized format\")\n",
    "parser.add_argument(\"--labels\", help=\"HDFS path to MNIST labels in parallelized format\")\n",
    "parser.add_argument(\"--format\", help=\"example format\", choices=[\"csv\",\"pickle\",\"tfr\"], default=\"csv\")\n",
    "parser.add_argument(\"--model\", help=\"HDFS path to save/load model during train/test\", default=\"mnist_model\")\n",
    "parser.add_argument(\"--readers\", help=\"number of reader/enqueue threads\", type=int, default=1)\n",
    "parser.add_argument(\"--steps\", help=\"maximum number of steps\", type=int, default=500)\n",
    "parser.add_argument(\"--batch_size\", help=\"number of examples per batch\", type=int, default=100)\n",
    "parser.add_argument(\"--mode\", help=\"train|inference\", default=\"train\")\n",
    "parser.add_argument(\"--rdma\", help=\"use rdma connection\", default=False)\n",
    "num_executors = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images_files = \"hdfs://10.110.18.217:8020/user/root/mnist/csv/train/images\"\n",
    "train_labels_files = \"hdfs://10.110.18.217:8020/user/root/mnist/csv/train/labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = parser.parse_args(['--mode', 'train', '--steps', '3000', '--epochs', '1',\n",
    "                          '--images', train_images_files, \n",
    "                          '--labels', train_labels_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:46:39 INFO:Reserving TFSparkNodes w/ TensorBoard\n",
      "07:46:39 INFO:listening for reservations at ('idap-agent-218.idap.com', 32967)\n",
      "07:46:39 INFO:Starting TensorFlow on executors\n",
      "07:46:39 INFO:Waiting for TFSparkNodes to start\n",
      "07:46:39 INFO:waiting for 3 reservations\n",
      "07:46:40 INFO:waiting for 3 reservations\n",
      "07:46:41 INFO:all reservations completed\n",
      "07:46:41 INFO:All TFSparkNodes started\n",
      "07:46:41 INFO:{'addr': '/tmp/pymp-NFrXpy/listener-iZ0hvY', 'task_index': 0, 'port': 33074, 'authkey': 'Y\\xa3SC\\xcf\\rD\\xf7\\x81Yx\\xe1\\x02w9\\xfd', 'worker_num': 1, 'host': 'idap-server-216.idap.com', 'ppid': 7294, 'job_name': 'worker', 'tb_pid': 7314, 'tb_port': 49400}\n",
      "07:46:41 INFO:{'addr': ('idap-agent-217.idap.com', 41033), 'task_index': 0, 'port': 44425, 'authkey': '\\xce2\\xb4J\\xf5*Oz\\xb3\\x0c\\xf6\\x7f\\xb6i\\xf1\\xf8', 'worker_num': 0, 'host': 'idap-agent-217.idap.com', 'ppid': 26458, 'job_name': 'ps', 'tb_pid': 0, 'tb_port': 0}\n",
      "07:46:41 INFO:{'addr': '/tmp/pymp-IMZUQf/listener-bZZZkw', 'task_index': 1, 'port': 56001, 'authkey': '\\xf9\\xfai)\\xe5LCY\\xb2\\x96\\x0c\\xf6\\x8c\\x80S\\xae', 'worker_num': 2, 'host': 'idap-agent-218.idap.com', 'ppid': 16705, 'job_name': 'worker', 'tb_pid': 0, 'tb_port': 0}\n",
      "07:46:41 INFO:========================================================================================\n",
      "07:46:41 INFO:\n",
      "07:46:41 INFO:TensorBoard running at:       http://idap-server-216.idap.com:49400\n",
      "07:46:41 INFO:\n",
      "07:46:41 INFO:========================================================================================\n"
     ]
    }
   ],
   "source": [
    "cluster = TFCluster.run(sc, mnist_dist.map_fun, args, num_executors, 1, True, TFCluster.InputMode.SPARK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:46:45 INFO:Feeding training data\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "images = sc.textFile(args.images).map(lambda ln: [int(x) for x in ln.split(',')])\n",
    "labels = sc.textFile(args.labels).map(lambda ln: [float(x) for x in ln.split(',')])\n",
    "dataRDD = images.zip(labels)\n",
    "cluster.train(dataRDD, args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:47:28 INFO:Stopping TensorFlow nodes\n",
      "07:47:28 INFO:Shutting down cluster\n"
     ]
    }
   ],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_images_files = \"hdfs://10.110.18.217:8020/user/root/mnist/csv/test/images\"\n",
    "test_labels_files = \"hdfs://10.110.18.217:8020/user/root/mnist/csv/test/labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parse arguments for inference\n",
    "args = parser.parse_args(['--mode', 'inference', \n",
    "                          '--images', test_images_files, \n",
    "                          '--labels', test_labels_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:47:33 INFO:Reserving TFSparkNodes \n",
      "07:47:33 INFO:listening for reservations at ('idap-agent-218.idap.com', 42731)\n",
      "07:47:33 INFO:Starting TensorFlow on executors\n",
      "07:47:34 INFO:Waiting for TFSparkNodes to start\n",
      "07:47:34 INFO:waiting for 3 reservations\n",
      "07:47:35 INFO:all reservations completed\n",
      "07:47:35 INFO:All TFSparkNodes started\n",
      "07:47:35 INFO:{'addr': '/tmp/pymp-wS9V8W/listener-aTau95', 'task_index': 1, 'port': 54526, 'authkey': '\\x8bJ\\xc35\\x16\\x8aB\\x00\\xbc\\xc0$\\\\Q\\xfb\\xe8\\xc3', 'worker_num': 2, 'host': 'idap-server-216.idap.com', 'ppid': 7294, 'job_name': 'worker', 'tb_pid': 0, 'tb_port': 0}\n",
      "07:47:35 INFO:{'addr': '/tmp/pymp-4rCYEk/listener-luPchF', 'task_index': 0, 'port': 49621, 'authkey': '_\\xd1\\x1b\\xbf\\x9e\\x1bG\\xd5\\x88t\\xb3LXO\\xce\\xef', 'worker_num': 1, 'host': 'idap-agent-218.idap.com', 'ppid': 16705, 'job_name': 'worker', 'tb_pid': 0, 'tb_port': 0}\n",
      "07:47:35 INFO:{'addr': ('idap-agent-217.idap.com', 57144), 'task_index': 0, 'port': 40200, 'authkey': '\\xd0\\xc5\\xa4\\xbd\\xc5\\x9f@\\x04\\xb5m%R\\xb5\\x97c\\r', 'worker_num': 0, 'host': 'idap-agent-217.idap.com', 'ppid': 26458, 'job_name': 'ps', 'tb_pid': 0, 'tb_port': 0}\n"
     ]
    }
   ],
   "source": [
    "cluster = TFCluster.run(sc, mnist_dist.map_fun, args, num_executors, 1, False, TFCluster.InputMode.SPARK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:47:35 INFO:Feeding inference data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2017-09-04T19:47:46.427081 Label: 7, Prediction: 7',\n",
       " '2017-09-04T19:47:46.427149 Label: 2, Prediction: 2',\n",
       " '2017-09-04T19:47:46.427167 Label: 1, Prediction: 1',\n",
       " '2017-09-04T19:47:46.427177 Label: 0, Prediction: 0',\n",
       " '2017-09-04T19:47:46.427187 Label: 4, Prediction: 4',\n",
       " '2017-09-04T19:47:46.427198 Label: 1, Prediction: 1',\n",
       " '2017-09-04T19:47:46.427207 Label: 4, Prediction: 4',\n",
       " '2017-09-04T19:47:46.427217 Label: 9, Prediction: 9',\n",
       " '2017-09-04T19:47:46.427227 Label: 5, Prediction: 6',\n",
       " '2017-09-04T19:47:46.427237 Label: 9, Prediction: 9',\n",
       " '2017-09-04T19:47:46.427247 Label: 0, Prediction: 0',\n",
       " '2017-09-04T19:47:46.427256 Label: 6, Prediction: 6',\n",
       " '2017-09-04T19:47:46.427265 Label: 9, Prediction: 9',\n",
       " '2017-09-04T19:47:46.427275 Label: 0, Prediction: 0',\n",
       " '2017-09-04T19:47:46.427294 Label: 1, Prediction: 1',\n",
       " '2017-09-04T19:47:46.427339 Label: 5, Prediction: 5',\n",
       " '2017-09-04T19:47:46.427351 Label: 9, Prediction: 9',\n",
       " '2017-09-04T19:47:46.427362 Label: 7, Prediction: 7',\n",
       " '2017-09-04T19:47:46.427371 Label: 3, Prediction: 3',\n",
       " '2017-09-04T19:47:46.427381 Label: 4, Prediction: 4']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare data as Spark RDD\n",
    "images = sc.textFile(args.images).map(lambda ln: [int(x) for x in ln.split(',')])\n",
    "labels = sc.textFile(args.labels).map(lambda ln: [float(x) for x in ln.split(',')])\n",
    "dataRDD = images.zip(labels)\n",
    "#feed data for inference\n",
    "prediction_results = cluster.inference(dataRDD)\n",
    "prediction_results.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:47:47 INFO:Stopping TensorFlow nodes\n",
      "07:47:47 INFO:Shutting down cluster\n"
     ]
    }
   ],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
